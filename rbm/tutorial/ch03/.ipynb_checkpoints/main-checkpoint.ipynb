{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, theano\n",
    "import numpy as np\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('mnist.pkl', 'rb') as f:\n",
    "    data = pickle.load(f, encoding='latin1')\n",
    "    g = open('mnist_update.pkl', 'wb')\n",
    "    pickle.dump(data, g, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('mnist_update.pkl', 'rb')\n",
    "train_set, valid_set, test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shared_dataset(dataxy):\n",
    "    data_x, data_y = dataxy\n",
    "    shared_x = theano.shared(np.asarray(data_x, dtype=theano.config.floatX))\n",
    "    shared_y = theano.shared(np.asarray(data_y, dtype=theano.config.floatX))\n",
    "    return shared_x, T.cast(shared_y, 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_set_x, test_set_y = shared_dataset(test_set)\n",
    "valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = train_set_x[2 * batch_size : 3 * batch_size]\n",
    "label = train_set_y[2 * batch_size : 3 * batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_one_loss = T.sum(T.neq(T.argmax(p_y_given_x), y))\n",
    "NNL = -T.sum(T.log(p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "d_loss_wrt_params = T.grad(loss, params)\n",
    "updates = [(params, params - learning_rate * d_loss_wrt_params)]\n",
    "MSGD = theano.function([x_batch, y_batch], loss, updates=updates)\n",
    "\n",
    "for (x_batch, y_batch) in train_batches:\n",
    "    print('Current loss is ', MSGD(x_batch, y_batch))\n",
    "\n",
    "L1 = T.sum(abs(params))\n",
    "L2_sqr = T.sum(params**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patience = 5000 # look as this many examples regardless\n",
    "patience_increase = 2 # wait this much longer when a new best is found\n",
    "improvement_threshold = 0.995 # a relative improvement of this much is considered significant\n",
    "validation_frequency = min(n_train_batches, patience/2) # go through this many minibatches before checking the network\n",
    "                                    # on the validation set; in this case, we check every epoch\n",
    "best_params = None\n",
    "best_validation_loss = np.inf\n",
    "test_score = 0.\n",
    "start_time = time.clock()\n",
    "\n",
    "done_looping = False\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "while (epoch < n_epochs) and (not done_looping):\n",
    "    epoch += 1\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "        d_loss_wrt_params =\n",
    "        params -= learning_rate * d_loss_wrt_params\n",
    "        \n",
    "        iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "        if (iter + 1) % validation_frequency ==0:\n",
    "            this_validation_loss = \n",
    "            if this_validation_loss < best_validation_loss:\n",
    "                if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                    patience = max(patience, iter * patience_increase)\n",
    "                best_params = copy.deepcopy(params)\n",
    "                best_validation_loss = this_validation_loss\n",
    "        if patience <= iter:\n",
    "            done_looping = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file = open('path', 'wb')\n",
    "pickle.dump(w.get_value(borrow=True), save_file, -1)\n",
    "pickle.dump(v.get_value(borrow=True), save_file, -1)\n",
    "pickle.dump(u.get_value(borrow=True), save_file, -1)\n",
    "save_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
